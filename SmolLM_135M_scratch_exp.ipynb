{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cc460c-2732-42e8-94ee-327b92d67bf3",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fd90ec-4335-4f71-a1c4-f86d5c170bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install safetensors torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b272700e-6180-4592-ac8c-5cef2b9bf825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu124\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.0+cu130)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in ./.venv/lib/python3.12/site-packages (from torch) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in ./.venv/lib/python3.12/site-packages (from torch) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in ./.venv/lib/python3.12/site-packages (from torch) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in ./.venv/lib/python3.12/site-packages (from torch) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in ./.venv/lib/python3.12/site-packages (from torch) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/nightly/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c529f-320c-47bd-9b6c-3ba03adfc2a1",
   "metadata": {},
   "source": [
    "Load your config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be5a755-78d9-4dda-a2bc-f25c214d4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(architectures=['LlamaForCausalLM'],\n",
       "          attention_bias=False,\n",
       "          attention_dropout=0.0,\n",
       "          bos_token_id=0,\n",
       "          eos_token_id=0,\n",
       "          hidden_act='silu',\n",
       "          hidden_size=576,\n",
       "          initializer_range=0.041666666666666664,\n",
       "          intermediate_size=1536,\n",
       "          is_llama_config=True,\n",
       "          max_position_embeddings=8192,\n",
       "          model_type='llama',\n",
       "          num_attention_heads=9,\n",
       "          num_hidden_layers=30,\n",
       "          num_key_value_heads=3,\n",
       "          pretraining_tp=1,\n",
       "          rms_norm_eps=1e-05,\n",
       "          rope_interleaved=False,\n",
       "          rope_scaling=None,\n",
       "          rope_theta=100000,\n",
       "          tie_word_embeddings=True,\n",
       "          torch_dtype='bfloat16',\n",
       "          transformers_version='4.40.1',\n",
       "          use_cache=True,\n",
       "          vocab_size=49152)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, types\n",
    "\n",
    "CONFIG_PATH = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\"   # <- change this\n",
    "MODEL_DIR   = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"              # folder containing model.safetensors\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg_dict = json.load(f)\n",
    "\n",
    "# turn into attribute-style object\n",
    "config = types.SimpleNamespace(**cfg_dict)\n",
    "\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea870f-b28d-464a-bec3-0f37cc23bda2",
   "metadata": {},
   "source": [
    "Core building blocks (RMSNorm + RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39a19ab-4715-4381-98a3-80a07bf1aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192160c5-b965-411e-a0a9-9669247e3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7dae64-a78c-493e-b308-1f2db51011ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513242a-7a7d-4fb3-9990-12daa1bed8e4",
   "metadata": {},
   "source": [
    "Attention with GQA + causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49688290-abf5-463e-a72b-c36b28f63671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = config.attention_dropout\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,C)\n",
    "        attention_mask: optional (B,1,T,T) additive mask with -inf on blocked positions\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)   # (B,h,T,d)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "\n",
    "        # RoPE\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # GQA: repeat kv to match q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,h,T,d)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B,h,T,T)\n",
    "\n",
    "        # causal mask\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        # optional extra mask (padding, etc.)\n",
    "        #if attention_mask is not None:\n",
    "        #    attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = F.dropout(attn_probs, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # (B,h,T,d)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25259245-b71b-43cb-88a3-66ba56737d30",
   "metadata": {},
   "source": [
    "MLP + Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd2a4e8-0372-4527-a121-abd699b64ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c7e83d-aa57-4b0c-a403-7a74fe3fed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489663e-c054-4017-ba46-13eb69c73687",
   "metadata": {},
   "source": [
    "Full LLaMA Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53fa14e8-93e0-4278-a455-a17e933021b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4a4a1-68ec-43b5-9b62-90176f9a2f18",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469af58f-903f-4b7e-8aac-63c58a1da2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GB10\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0724aec-284f-4021-8c11-89b9b687d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134.515008"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1182464b-6093-4312-80c9-eb5deb35cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ef1ff450973e3e25306fd39150a5362c\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import torch\n",
    "\n",
    "def tensor_hash(t):\n",
    "    # Convert BF16 -> FP32 because numpy doesn't support BF16\n",
    "    arr = t.detach().cpu().to(torch.float32).numpy()\n",
    "    return hashlib.md5(arr.tobytes()).hexdigest()\n",
    "\n",
    "# Pick a specific layer\n",
    "before_hash = tensor_hash(model.model.layers[0].self_attn.q_proj.weight)\n",
    "print(\"Before:\", before_hash)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440e40e-5f86-4a05-a00b-d4a417eb10aa",
   "metadata": {},
   "source": [
    "Load weights from model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90a23b70-dd95-4798-bcc2-6edc1e70c06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: tensor([-0.0152,  0.0018,  0.0300,  0.0149, -0.0356], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: 1\n",
      "Unexpected keys: 0\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae3778b9-c130-4075-8ef8-ee687dd723c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n"
     ]
    }
   ],
   "source": [
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c78cf-fc09-4410-9786-66180b524d4d",
   "metadata": {},
   "source": [
    "Sanity test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55e3357e-46eb-411b-827a-3be2179843b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bb0cf2d-49a0-4c1c-9fa2-8fdef266d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c48c32-f989-4f5c-b25c-c98e91416aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a43c0879-586c-4f6d-ab9f-b26e954a9646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 341094\n",
      "Chunks: 42\n",
      "Output shape: (tensor([[[ 17.3750,   3.5625,   3.6562,  ...,  11.9375,  13.6875,  13.7500],\n",
      "         [ 10.5000,  -6.6562,  -6.5938,  ...,  -2.1719,   2.9531,   0.7617],\n",
      "         [  4.5625, -12.8750, -12.8125,  ...,  -4.1250,   1.8594,  -3.4688],\n",
      "         ...,\n",
      "         [ 21.5000,   8.9375,   9.0625,  ...,  14.3125,  13.6250,  14.6875],\n",
      "         [ 23.3750,   8.4375,   8.5000,  ...,  15.0000,  15.2500,  15.5000],\n",
      "         [ 21.5000,   6.9062,   7.0000,  ...,  13.1875,  13.6250,  14.0000]],\n",
      "\n",
      "        [[  9.4375,  -1.7422,  -1.6250,  ...,   7.4688,   8.3125,   4.5938],\n",
      "         [  7.2500,  -3.7344,  -3.6406,  ...,   1.3281,   4.1250,   1.7969],\n",
      "         [ 18.6250,   2.1094,   2.2500,  ...,  11.0000,  12.9375,  10.5000],\n",
      "         ...,\n",
      "         [ 21.7500,   6.7188,   6.7812,  ...,  13.1250,  14.3125,  12.4375],\n",
      "         [ 20.6250,   3.8906,   3.9688,  ...,  11.1875,  12.0000,   9.6875],\n",
      "         [ 15.8750,   1.1953,   1.2656,  ...,   9.1250,   9.9375,   7.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "SEQ_LEN = config.max_position_embeddings  # 8192\n",
    "\n",
    "# 1. Tokenize\n",
    "input_ids = tokenizer(text).input_ids\n",
    "print(\"Total tokens:\", len(input_ids))\n",
    "\n",
    "# 2. Chunk\n",
    "chunks = [input_ids[i:i+SEQ_LEN] for i in range(0, len(input_ids), SEQ_LEN)]\n",
    "print(\"Chunks:\", len(chunks))\n",
    "\n",
    "# 3. Pad\n",
    "def pad(x):\n",
    "    return x + [tokenizer.pad_token_id or 0] * (SEQ_LEN - len(x))\n",
    "\n",
    "padded_chunks = [torch.tensor(pad(c)).long() for c in chunks]\n",
    "\n",
    "# 4. Batching\n",
    "batch_size = 2\n",
    "batches = [\n",
    "    torch.stack(padded_chunks[i:i+batch_size])\n",
    "    for i in range(0, len(padded_chunks), batch_size)\n",
    "]\n",
    "\n",
    "# 5. Forward pass\n",
    "batch = batches[0].cuda()   # (2, 8192)\n",
    "with torch.no_grad():\n",
    "    out = model(batch)\n",
    "\n",
    "print(\"Output shape:\", out)  # (2, 8192, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6a6fe0b-1aa7-43d5-b440-d919bba2c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8249)\n"
     ]
    }
   ],
   "source": [
    "print(-torch.log(torch.tensor(1/50257)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b9a95be-8297-48ff-bfa5-fdf0b6d4d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b268428c-644a-41ea-81a1-4d2098b7c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 440.0421 | dt: 3281.37ms | tok/sec: 2496.52\n",
      "step 1 | loss: 131.2214 | dt: 2571.89ms | tok/sec: 3185.21\n",
      "step 2 | loss: 85.2180 | dt: 2567.72ms | tok/sec: 3190.38\n",
      "step 3 | loss: 87.4907 | dt: 2572.76ms | tok/sec: 3184.12\n",
      "step 4 | loss: 88.0964 | dt: 2574.11ms | tok/sec: 3182.45\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)   # now it works!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54f90105-5155-45ac-876b-691a314fe0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0173, -0.0118, -0.0225,  0.0128, -0.0002], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 424.2340 | dt: 2165.38ms | tok/sec: 3783.18\n",
      "step 1 | loss: 121.4082 | dt: 2157.61ms | tok/sec: 3796.80\n",
      "step 2 | loss: 83.0007 | dt: 2154.49ms | tok/sec: 3802.30\n",
      "step 3 | loss: 81.3026 | dt: 2158.36ms | tok/sec: 3795.48\n",
      "step 4 | loss: 79.0683 | dt: 2159.38ms | tok/sec: 3793.68\n"
     ]
    }
   ],
   "source": [
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = config.attention_dropout\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,C)\n",
    "        attention_mask: optional (B,1,T,T) additive mask with -inf on blocked positions\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)   # (B,h,T,d)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "\n",
    "        # RoPE\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # GQA: repeat kv to match q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,h,T,d)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B,h,T,T)\n",
    "\n",
    "        # causal mask\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        # optional extra mask (padding, etc.)\n",
    "        #if attention_mask is not None:\n",
    "        #    attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = F.dropout(attn_probs, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # (B,h,T,d)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)   # now it works!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6374d27-f192-48fb-a133-3478f63af56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0119, -0.0312, -0.0110,  0.0171,  0.0204], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 435.6360 | dt: 1701.36ms | tok/sec: 4814.96\n",
      "step 1 | loss: 127.3702 | dt: 1692.46ms | tok/sec: 4840.30\n",
      "step 2 | loss: 82.7005 | dt: 1697.36ms | tok/sec: 4826.32\n",
      "step 3 | loss: 81.7776 | dt: 1693.87ms | tok/sec: 4836.27\n",
      "step 4 | loss: 80.6131 | dt: 1697.14ms | tok/sec: 4826.94\n"
     ]
    }
   ],
   "source": [
    "# added autocast\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = config.attention_dropout\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,C)\n",
    "        attention_mask: optional (B,1,T,T) additive mask with -inf on blocked positions\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)   # (B,h,T,d)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "\n",
    "        # RoPE\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # GQA: repeat kv to match q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,h,T,d)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B,h,T,T)\n",
    "\n",
    "        # causal mask\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        # optional extra mask (padding, etc.)\n",
    "        #if attention_mask is not None:\n",
    "        #    attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = F.dropout(attn_probs, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # (B,h,T,d)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dffed0a8-d800-4edd-a9ac-53d0cd303750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0282,  0.0075,  0.0056, -0.0366, -0.0371], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 437.2581 | dt: 650.46ms | tok/sec: 12594.08\n",
      "step 1 | loss: 128.7806 | dt: 640.79ms | tok/sec: 12784.19\n",
      "step 2 | loss: 89.0142 | dt: 643.58ms | tok/sec: 12728.78\n",
      "step 3 | loss: 87.2735 | dt: 640.43ms | tok/sec: 12791.35\n",
      "step 4 | loss: 86.6007 | dt: 640.86ms | tok/sec: 12782.75\n"
     ]
    }
   ],
   "source": [
    "# changed the Attention block with SDPA\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d44645-c288-43f6-b7b3-61ff13b74e83",
   "metadata": {},
   "source": [
    "\n",
    "InductorError: CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp8ug7vhc2/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp8ug7vhc2/cuda_utils.cpython-312-aarch64-linux-gnu.so', '-lcuda', '-L/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/lib/aarch64-linux-gnu', '-I/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp8ug7vhc2', '-I/usr/include/python3.12']' returned non-zero exit status 1.\n",
    "\n",
    "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
    "\n",
    "🧨 Why torch.compile() fails on your DGX Spark?\n",
    "NVIDIA DGX Spark uses:\n",
    "\n",
    "✔ ARM CPU (AArch64)\n",
    "✔ Blackwell GPU\n",
    "❌ Triton compiler is x86_64 only\n",
    "❌ PyTorch Inductor requires Triton\n",
    "❌ Triton CUDA backend cannot compile kernel modules on ARM\n",
    "\n",
    "So torch.compile() cannot generate fused kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42b477d9-3fb0-48d2-9799-6a04b8389643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print(hasattr(F, \"rms_norm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c3e7c9c-ea0e-44cd-92fb-2221d84f0dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0415,  0.0033, -0.0291,  0.0155, -0.0041], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 434.6060 | dt: 566.15ms | tok/sec: 14469.57\n",
      "step 1 | loss: 127.9871 | dt: 536.08ms | tok/sec: 15281.41\n",
      "step 2 | loss: 87.6432 | dt: 535.95ms | tok/sec: 15284.89\n",
      "step 3 | loss: 83.6997 | dt: 537.94ms | tok/sec: 15228.52\n",
      "step 4 | loss: 82.9705 | dt: 535.74ms | tok/sec: 15291.08\n"
     ]
    }
   ],
   "source": [
    "# replaced RMSNorms code\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24547155-74ee-42e8-bd4f-707861f70497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0032,  0.0308, -0.0349, -0.0388,  0.0028], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 432.9732 | dt: 544.42ms | tok/sec: 15047.07\n",
      "step 1 | loss: 125.1638 | dt: 528.67ms | tok/sec: 15495.37\n",
      "step 2 | loss: 89.2411 | dt: 532.10ms | tok/sec: 15395.54\n",
      "step 3 | loss: 89.6816 | dt: 535.72ms | tok/sec: 15291.51\n",
      "step 4 | loss: 86.6060 | dt: 529.72ms | tok/sec: 15464.65\n"
     ]
    }
   ],
   "source": [
    "# changing \n",
    "'''\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "'''\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4997e3cf-3a55-4869-9968-302e8f308537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0305,  0.0203, -0.0315,  0.0049, -0.0312], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 10 batches\n",
      "step 0 | loss: 433.2917 | dt: 3884.80ms | tok/sec: 8434.92\n",
      "step 1 | loss: 124.0382 | dt: 2216.75ms | tok/sec: 14781.97\n",
      "step 2 | loss: 89.7909 | dt: 2138.37ms | tok/sec: 15323.81\n",
      "step 3 | loss: 86.3727 | dt: 2136.66ms | tok/sec: 15336.08\n",
      "step 4 | loss: 85.2940 | dt: 2126.21ms | tok/sec: 15411.43\n"
     ]
    }
   ],
   "source": [
    "# increasing the batch size x2\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "# trial batch size 16 - same speed like before awesome\n",
    "# trial batch size 32 - same speed like before awesome\n",
    "# trial batch size 64 - hanging after 1st iteration.\n",
    "\n",
    "train_loader = DataLoaderLite(B=32, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a32efd8-d693-493d-89dd-b0455a6bf97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0129,  0.0052, -0.0114,  0.0248,  0.0078], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 20 batches\n",
      "step 0 | loss: 109.6663 | dt: 1022.70ms | tok/sec: 16020.29\n",
      "step 1 | loss: 108.7750 | dt: 1019.81ms | tok/sec: 16065.69\n",
      "step 2 | loss: 108.2115 | dt: 1022.03ms | tok/sec: 16030.81\n",
      "step 3 | loss: 109.1347 | dt: 1070.70ms | tok/sec: 15302.21\n",
      "step 4 | loss: 31.8584 | dt: 1020.77ms | tok/sec: 16050.56\n",
      "step 5 | loss: 32.9130 | dt: 1028.16ms | tok/sec: 15935.28\n",
      "step 6 | loss: 34.3935 | dt: 1026.78ms | tok/sec: 15956.70\n",
      "step 7 | loss: 33.2719 | dt: 1069.57ms | tok/sec: 15318.26\n",
      "step 8 | loss: 21.2987 | dt: 1031.79ms | tok/sec: 15879.20\n",
      "step 9 | loss: 21.3444 | dt: 1013.05ms | tok/sec: 16172.91\n"
     ]
    }
   ],
   "source": [
    "# increasing the batch size x2 + grad_accum = 4 \n",
    "''' NOT MUCH difference'''\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "# trial batch size 16 - same speed like before awesome\n",
    "# trial batch size 32 - same speed like before awesome\n",
    "# trial batch size 64 - hanging after 1st iteration.\n",
    "\n",
    "train_loader = DataLoaderLite(B=16, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "grad_accum = 4\n",
    "optimizer.zero_grad()\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss/grad_accum\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    #optimizer.step()\n",
    "\n",
    "    if ((i+1) % grad_accum) == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23ef70e0-d97b-4ea3-8c5a-61f654e5edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62e6a695-53e1-4a20-b537-6f6d0674945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      aten::empty         1.97%       1.424ms         1.97%       1.424ms       6.748us         480 B         480 B           211  \n",
      "                                  aten::embedding         0.06%      43.329us         0.44%     317.185us     317.185us           0 B           0 B             1  \n",
      "                                    aten::reshape         1.65%       1.191ms         4.61%       3.335ms       8.509us           0 B           0 B           392  \n",
      "                                       aten::view         3.44%       2.488ms         3.44%       2.488ms       5.480us           0 B           0 B           454  \n",
      "                               aten::index_select         0.07%      54.160us         0.36%     256.688us     256.688us           0 B           0 B             1  \n",
      "                                    aten::resize_         0.29%     206.752us         0.29%     206.752us       6.669us           0 B           0 B            31  \n",
      "                                     aten::expand         0.57%     415.296us         0.73%     524.576us       8.600us           0 B           0 B            61  \n",
      "                                 aten::as_strided         2.85%       2.061ms         2.85%       2.061ms       1.756us           0 B           0 B          1174  \n",
      "                                     aten::gather         0.16%     116.672us         0.22%     156.336us     156.336us           0 B           0 B             1  \n",
      "                                 cudaLaunchKernel        14.09%      10.188ms        14.09%      10.188ms       9.121us           0 B           0 B          1117  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 72.289ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model\n",
    "inputs = x\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30a6b031-d008-428f-91b5-ccd51d30d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         1.10%       2.481ms         3.05%       6.882ms      32.616us      90.088ms        38.83%     109.423ms     518.594us           211   2203318.223  \n",
      "                                              aten::mul         0.95%       2.140ms         2.86%       6.450ms      21.357us      50.611ms        21.81%      53.287ms     176.447us           302      1330.674  \n",
      "                                              aten::add         0.56%       1.268ms         1.14%       2.566ms      14.179us      21.746ms         9.37%      22.645ms     125.113us           181       472.359  \n",
      "                                        aten::embedding         0.01%      31.281us         0.11%     246.641us     246.641us       0.000us         0.00%     156.800us     156.800us             1            --  \n",
      "                                          aten::reshape         0.27%     613.361us         0.74%       1.681ms       4.287us       0.000us         0.00%       0.000us       0.000us           392            --  \n",
      "                                             aten::view         0.55%       1.238ms         0.55%       1.238ms       2.728us       0.000us         0.00%       0.000us       0.000us           454            --  \n",
      "                                     aten::index_select         0.01%      31.664us         0.09%     205.760us     205.760us       0.000us         0.00%     156.800us     156.800us             1            --  \n",
      "                                            aten::empty         0.34%     776.369us         0.34%     776.369us       3.679us       0.000us         0.00%       0.000us       0.000us           211            --  \n",
      "                                          aten::resize_         0.04%      94.801us         0.04%      94.801us       3.058us       0.000us         0.00%       0.000us       0.000us            31            --  \n",
      "                                           aten::expand         0.14%     306.688us         0.16%     368.640us       6.043us       0.000us         0.00%       0.000us       0.000us            61            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 225.758ms\n",
      "Self CUDA time total: 232.032ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    with_flops=True\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4d3db-a0cc-44f1-94f7-e7f5f0823f5a",
   "metadata": {},
   "source": [
    "The Memory and Compute Requirement notebook¶\n",
    "This notebook is a simple tool to estimate the memory and compute requirements for the solution of the problem. The notebook is divided into two sections: the first one is dedicated to the memory requirements, while the second one is dedicated to the compute requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96921a6f-bb31-4c0e-ae14-6710e203d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some working statistics\n",
    "\n",
    "# Model\n",
    "vocabulary_size = 50257\n",
    "embedding_dimensions = 768\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "feed_forward_ratio = 4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parameter_counts = {\n",
    "    \"Token Embeddings\": vocabulary_size * embedding_dimensions,\n",
    "    \"Attention\": (\n",
    "        embedding_dimensions * 3 * embedding_dimensions + embedding_dimensions**2\n",
    "    ) * num_hidden_layers,\n",
    "    \"MLP\": (\n",
    "        embedding_dimensions * feed_forward_ratio * embedding_dimensions * 2\n",
    "    ) * num_hidden_layers,\n",
    "    \"Norm\": embedding_dimensions * 2 * num_hidden_layers + embedding_dimensions,\n",
    "    \"Output\": 0, # We share the embedding weights\n",
    "}\n",
    "\n",
    "plt.bar(parameter_counts.keys(), parameter_counts.values())\n",
    "\n",
    "plt.title(\"Model Parameters\")\n",
    "plt.ylabel(\"# of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Total parameters: {total_parameter_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0c3f69-8720-4a33-a9eb-4710abc3a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings               28,311,552      19.16%\n",
      "Attention (QKV + O)            39,813,120      26.94%\n",
      "MLP                            79,626,240      53.88%\n",
      "Norms                              35,136       0.02%\n",
      "Output Head                             0       0.00%\n",
      "\n",
      "Total parameters: 147,786,048\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hidden = config.hidden_size\n",
    "heads = config.num_attention_heads\n",
    "layers = config.num_hidden_layers\n",
    "ffn = config.intermediate_size\n",
    "vocab = config.vocab_size\n",
    "tie = getattr(config, \"tie_word_embeddings\", False)\n",
    "\n",
    "parameter_counts = {\n",
    "    \"Token Embeddings\": vocab * hidden,\n",
    "    \n",
    "    # Q, K, V projections: hidden → (3 * hidden)\n",
    "    # Output projection: hidden → hidden\n",
    "    \"Attention (QKV + O)\": (hidden * hidden * 4) * layers,\n",
    "\n",
    "    # FFN (Gate + Up) + Down\n",
    "    \"MLP\": (hidden * ffn * 2 + ffn * hidden) * layers,\n",
    "\n",
    "    # RMSNorm layers\n",
    "    \"Norms\": (hidden * 2 * layers + hidden),\n",
    "\n",
    "    # LM head (if NOT tied)\n",
    "    \"Output Head\": 0 if tie else hidden * vocab,\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.bar(parameter_counts.keys(), parameter_counts.values())\n",
    "plt.title(\"Transformer Parameter Breakdown\")\n",
    "plt.ylabel(\"# of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Print table\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_parameter_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678dcf1d-2aa5-4754-9222-0cca3d955627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9cc460c-2732-42e8-94ee-327b92d67bf3",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fd90ec-4335-4f71-a1c4-f86d5c170bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install safetensors torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b272700e-6180-4592-ac8c-5cef2b9bf825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu124\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.0+cu130)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc==13.0.48 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-runtime==13.0.48 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cuda-cupti==13.0.48 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.48)\n",
      "Requirement already satisfied: nvidia-cudnn-cu13==9.13.0.50 in ./.venv/lib/python3.12/site-packages (from torch) (9.13.0.50)\n",
      "Requirement already satisfied: nvidia-cublas==13.0.0.19 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.0.19)\n",
      "Requirement already satisfied: nvidia-cufft==12.0.0.15 in ./.venv/lib/python3.12/site-packages (from torch) (12.0.0.15)\n",
      "Requirement already satisfied: nvidia-curand==10.4.0.35 in ./.venv/lib/python3.12/site-packages (from torch) (10.4.0.35)\n",
      "Requirement already satisfied: nvidia-cusolver==12.0.3.29 in ./.venv/lib/python3.12/site-packages (from torch) (12.0.3.29)\n",
      "Requirement already satisfied: nvidia-cusparse==12.6.2.49 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.2.49)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu13==0.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (0.8.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu13==2.27.7 in ./.venv/lib/python3.12/site-packages (from torch) (2.27.7)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu13==3.3.24 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.24)\n",
      "Requirement already satisfied: nvidia-nvtx==13.0.39 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-nvjitlink==13.0.39 in ./.venv/lib/python3.12/site-packages (from torch) (13.0.39)\n",
      "Requirement already satisfied: nvidia-cufile==1.15.0.42 in ./.venv/lib/python3.12/site-packages (from torch) (1.15.0.42)\n",
      "Requirement already satisfied: triton==3.5.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch --index-url https://download.pytorch.org/whl/nightly/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c529f-320c-47bd-9b6c-3ba03adfc2a1",
   "metadata": {},
   "source": [
    "Load your config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5a755-78d9-4dda-a2bc-f25c214d4191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(architectures=['LlamaForCausalLM'],\n",
       "          attention_bias=False,\n",
       "          attention_dropout=0.0,\n",
       "          bos_token_id=0,\n",
       "          eos_token_id=0,\n",
       "          hidden_act='silu',\n",
       "          hidden_size=576,\n",
       "          initializer_range=0.041666666666666664,\n",
       "          intermediate_size=1536,\n",
       "          is_llama_config=True,\n",
       "          max_position_embeddings=8192,\n",
       "          model_type='llama',\n",
       "          num_attention_heads=9,\n",
       "          num_hidden_layers=30,\n",
       "          num_key_value_heads=3,\n",
       "          pretraining_tp=1,\n",
       "          rms_norm_eps=1e-05,\n",
       "          rope_interleaved=False,\n",
       "          rope_scaling=None,\n",
       "          rope_theta=100000,\n",
       "          tie_word_embeddings=True,\n",
       "          torch_dtype='bfloat16',\n",
       "          transformers_version='4.40.1',\n",
       "          use_cache=True,\n",
       "          vocab_size=49152)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, types\n",
    "\n",
    "CONFIG_PATH = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\"   # <- change this\n",
    "MODEL_DIR   = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"              # folder containing model.safetensors\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg_dict = json.load(f)\n",
    "\n",
    "# turn into attribute-style object\n",
    "config = types.SimpleNamespace(**cfg_dict)\n",
    "\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea870f-b28d-464a-bec3-0f37cc23bda2",
   "metadata": {},
   "source": [
    "Core building blocks (RMSNorm + RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39a19ab-4715-4381-98a3-80a07bf1aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192160c5-b965-411e-a0a9-9669247e3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7dae64-a78c-493e-b308-1f2db51011ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513242a-7a7d-4fb3-9990-12daa1bed8e4",
   "metadata": {},
   "source": [
    "Attention with GQA + causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49688290-abf5-463e-a72b-c36b28f63671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = config.attention_dropout\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,C)\n",
    "        attention_mask: optional (B,1,T,T) additive mask with -inf on blocked positions\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)   # (B,h,T,d)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "\n",
    "        # RoPE\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # GQA: repeat kv to match q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,h,T,d)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B,h,T,T)\n",
    "\n",
    "        # causal mask\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        # optional extra mask (padding, etc.)\n",
    "        #if attention_mask is not None:\n",
    "        #    attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = F.dropout(attn_probs, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # (B,h,T,d)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25259245-b71b-43cb-88a3-66ba56737d30",
   "metadata": {},
   "source": [
    "MLP + Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd2a4e8-0372-4527-a121-abd699b64ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c7e83d-aa57-4b0c-a403-7a74fe3fed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489663e-c054-4017-ba46-13eb69c73687",
   "metadata": {},
   "source": [
    "Full LLaMA Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53fa14e8-93e0-4278-a455-a17e933021b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4a4a1-68ec-43b5-9b62-90176f9a2f18",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469af58f-903f-4b7e-8aac-63c58a1da2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GB10\n",
      "13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0724aec-284f-4021-8c11-89b9b687d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134.515008"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1182464b-6093-4312-80c9-eb5deb35cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ef1ff450973e3e25306fd39150a5362c\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import torch\n",
    "\n",
    "def tensor_hash(t):\n",
    "    # Convert BF16 -> FP32 because numpy doesn't support BF16\n",
    "    arr = t.detach().cpu().to(torch.float32).numpy()\n",
    "    return hashlib.md5(arr.tobytes()).hexdigest()\n",
    "\n",
    "# Pick a specific layer\n",
    "before_hash = tensor_hash(model.model.layers[0].self_attn.q_proj.weight)\n",
    "print(\"Before:\", before_hash)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440e40e-5f86-4a05-a00b-d4a417eb10aa",
   "metadata": {},
   "source": [
    "Load weights from model.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90a23b70-dd95-4798-bcc2-6edc1e70c06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: tensor([-0.0152,  0.0018,  0.0300,  0.0149, -0.0356], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: 1\n",
      "Unexpected keys: 0\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae3778b9-c130-4075-8ef8-ee687dd723c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n"
     ]
    }
   ],
   "source": [
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c78cf-fc09-4410-9786-66180b524d4d",
   "metadata": {},
   "source": [
    "Sanity test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55e3357e-46eb-411b-827a-3be2179843b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bb0cf2d-49a0-4c1c-9fa2-8fdef266d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35c48c32-f989-4f5c-b25c-c98e91416aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a43c0879-586c-4f6d-ab9f-b26e954a9646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 341094\n",
      "Chunks: 42\n",
      "Output shape: (tensor([[[ 17.3750,   3.5625,   3.6562,  ...,  11.9375,  13.6875,  13.7500],\n",
      "         [ 10.5000,  -6.6562,  -6.5938,  ...,  -2.1719,   2.9531,   0.7617],\n",
      "         [  4.5625, -12.8750, -12.8125,  ...,  -4.1250,   1.8594,  -3.4688],\n",
      "         ...,\n",
      "         [ 21.5000,   8.9375,   9.0625,  ...,  14.3125,  13.6250,  14.6875],\n",
      "         [ 23.3750,   8.4375,   8.5000,  ...,  15.0000,  15.2500,  15.5000],\n",
      "         [ 21.5000,   6.9062,   7.0000,  ...,  13.1875,  13.6250,  14.0000]],\n",
      "\n",
      "        [[  9.4375,  -1.7422,  -1.6250,  ...,   7.4688,   8.3125,   4.5938],\n",
      "         [  7.2500,  -3.7344,  -3.6406,  ...,   1.3281,   4.1250,   1.7969],\n",
      "         [ 18.6250,   2.1094,   2.2500,  ...,  11.0000,  12.9375,  10.5000],\n",
      "         ...,\n",
      "         [ 21.7500,   6.7188,   6.7812,  ...,  13.1250,  14.3125,  12.4375],\n",
      "         [ 20.6250,   3.8906,   3.9688,  ...,  11.1875,  12.0000,   9.6875],\n",
      "         [ 15.8750,   1.1953,   1.2656,  ...,   9.1250,   9.9375,   7.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "SEQ_LEN = config.max_position_embeddings  # 8192\n",
    "\n",
    "# 1. Tokenize\n",
    "input_ids = tokenizer(text).input_ids\n",
    "print(\"Total tokens:\", len(input_ids))\n",
    "\n",
    "# 2. Chunk\n",
    "chunks = [input_ids[i:i+SEQ_LEN] for i in range(0, len(input_ids), SEQ_LEN)]\n",
    "print(\"Chunks:\", len(chunks))\n",
    "\n",
    "# 3. Pad\n",
    "def pad(x):\n",
    "    return x + [tokenizer.pad_token_id or 0] * (SEQ_LEN - len(x))\n",
    "\n",
    "padded_chunks = [torch.tensor(pad(c)).long() for c in chunks]\n",
    "\n",
    "# 4. Batching\n",
    "batch_size = 2\n",
    "batches = [\n",
    "    torch.stack(padded_chunks[i:i+batch_size])\n",
    "    for i in range(0, len(padded_chunks), batch_size)\n",
    "]\n",
    "\n",
    "# 5. Forward pass\n",
    "batch = batches[0].cuda()   # (2, 8192)\n",
    "with torch.no_grad():\n",
    "    out = model(batch)\n",
    "\n",
    "print(\"Output shape:\", out)  # (2, 8192, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6a6fe0b-1aa7-43d5-b440-d919bba2c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8249)\n"
     ]
    }
   ],
   "source": [
    "print(-torch.log(torch.tensor(1/50257)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b9a95be-8297-48ff-bfa5-fdf0b6d4d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b268428c-644a-41ea-81a1-4d2098b7c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 440.0421 | dt: 3281.37ms | tok/sec: 2496.52\n",
      "step 1 | loss: 131.2214 | dt: 2571.89ms | tok/sec: 3185.21\n",
      "step 2 | loss: 85.2180 | dt: 2567.72ms | tok/sec: 3190.38\n",
      "step 3 | loss: 87.4907 | dt: 2572.76ms | tok/sec: 3184.12\n",
      "step 4 | loss: 88.0964 | dt: 2574.11ms | tok/sec: 3182.45\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)   # now it works!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54f90105-5155-45ac-876b-691a314fe0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0173, -0.0118, -0.0225,  0.0128, -0.0002], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 424.2340 | dt: 2165.38ms | tok/sec: 3783.18\n",
      "step 1 | loss: 121.4082 | dt: 2157.61ms | tok/sec: 3796.80\n",
      "step 2 | loss: 83.0007 | dt: 2154.49ms | tok/sec: 3802.30\n",
      "step 3 | loss: 81.3026 | dt: 2158.36ms | tok/sec: 3795.48\n",
      "step 4 | loss: 79.0683 | dt: 2159.38ms | tok/sec: 3793.68\n"
     ]
    }
   ],
   "source": [
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = config.attention_dropout\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,C)\n",
    "        attention_mask: optional (B,1,T,T) additive mask with -inf on blocked positions\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)   # (B,h,T,d)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "\n",
    "        # RoPE\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # GQA: repeat kv to match q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,h,T,d)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B,h,T,T)\n",
    "\n",
    "        # causal mask\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        # optional extra mask (padding, etc.)\n",
    "        #if attention_mask is not None:\n",
    "        #    attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = F.dropout(attn_probs, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # (B,h,T,d)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)   # now it works!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6374d27-f192-48fb-a133-3478f63af56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0119, -0.0312, -0.0110,  0.0171,  0.0204], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 435.6360 | dt: 1701.36ms | tok/sec: 4814.96\n",
      "step 1 | loss: 127.3702 | dt: 1692.46ms | tok/sec: 4840.30\n",
      "step 2 | loss: 82.7005 | dt: 1697.36ms | tok/sec: 4826.32\n",
      "step 3 | loss: 81.7776 | dt: 1693.87ms | tok/sec: 4836.27\n",
      "step 4 | loss: 80.6131 | dt: 1697.14ms | tok/sec: 4826.94\n"
     ]
    }
   ],
   "source": [
    "# added autocast\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = config.attention_dropout\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        \"\"\"\n",
    "        x: (B,T,C)\n",
    "        attention_mask: optional (B,1,T,T) additive mask with -inf on blocked positions\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1,2)   # (B,h,T,d)\n",
    "        k = self.k_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "        v = self.v_proj(x).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2) # (B,kv,T,d)\n",
    "\n",
    "        # RoPE\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q, sin, cos)\n",
    "        k = apply_rope(k, sin, cos)\n",
    "\n",
    "        # GQA: repeat kv to match q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,h,T,d)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)  # (B,h,T,T)\n",
    "\n",
    "        # causal mask\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        # optional extra mask (padding, etc.)\n",
    "        #if attention_mask is not None:\n",
    "        #    attn_scores = attn_scores + attention_mask\n",
    "\n",
    "        causal = torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "        attn_scores = attn_scores.masked_fill(causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        attn_probs = F.dropout(attn_probs, p=self.attn_dropout, training=self.training)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # (B,h,T,d)\n",
    "        out = out.transpose(1,2).contiguous().view(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dffed0a8-d800-4edd-a9ac-53d0cd303750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0282,  0.0075,  0.0056, -0.0366, -0.0371], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 437.2581 | dt: 650.46ms | tok/sec: 12594.08\n",
      "step 1 | loss: 128.7806 | dt: 640.79ms | tok/sec: 12784.19\n",
      "step 2 | loss: 89.0142 | dt: 643.58ms | tok/sec: 12728.78\n",
      "step 3 | loss: 87.2735 | dt: 640.43ms | tok/sec: 12791.35\n",
      "step 4 | loss: 86.6007 | dt: 640.86ms | tok/sec: 12782.75\n"
     ]
    }
   ],
   "source": [
    "# changed the Attention block with SDPA\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,T,C)\n",
    "        norm = x.pow(2).mean(-1, keepdim=True)\n",
    "        return x * torch.rsqrt(norm + self.eps) * self.weight\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d44645-c288-43f6-b7b3-61ff13b74e83",
   "metadata": {},
   "source": [
    "\n",
    "InductorError: CalledProcessError: Command '['/usr/bin/gcc', '/tmp/tmp8ug7vhc2/cuda_utils.c', '-O3', '-shared', '-fPIC', '-Wno-psabi', '-o', '/tmp/tmp8ug7vhc2/cuda_utils.cpython-312-aarch64-linux-gnu.so', '-lcuda', '-L/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/triton/backends/nvidia/lib', '-L/lib/aarch64-linux-gnu', '-I/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/triton/backends/nvidia/include', '-I/tmp/tmp8ug7vhc2', '-I/usr/include/python3.12']' returned non-zero exit status 1.\n",
    "\n",
    "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
    "\n",
    "🧨 Why torch.compile() fails on your DGX Spark?\n",
    "NVIDIA DGX Spark uses:\n",
    "\n",
    "✔ ARM CPU (AArch64)\n",
    "✔ Blackwell GPU\n",
    "❌ Triton compiler is x86_64 only\n",
    "❌ PyTorch Inductor requires Triton\n",
    "❌ Triton CUDA backend cannot compile kernel modules on ARM\n",
    "\n",
    "So torch.compile() cannot generate fused kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "42b477d9-3fb0-48d2-9799-6a04b8389643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "print(hasattr(F, \"rms_norm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c3e7c9c-ea0e-44cd-92fb-2221d84f0dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0415,  0.0033, -0.0291,  0.0155, -0.0041], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 434.6060 | dt: 566.15ms | tok/sec: 14469.57\n",
      "step 1 | loss: 127.9871 | dt: 536.08ms | tok/sec: 15281.41\n",
      "step 2 | loss: 87.6432 | dt: 535.95ms | tok/sec: 15284.89\n",
      "step 3 | loss: 83.6997 | dt: 537.94ms | tok/sec: 15228.52\n",
      "step 4 | loss: 82.9705 | dt: 535.74ms | tok/sec: 15291.08\n"
     ]
    }
   ],
   "source": [
    "# replaced RMSNorms code\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
    "torch.backends.cudnn.fp32_precision = \"tf32\"\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24547155-74ee-42e8-bd4f-707861f70497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0032,  0.0308, -0.0349, -0.0388,  0.0028], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "step 0 | loss: 432.9732 | dt: 544.42ms | tok/sec: 15047.07\n",
      "step 1 | loss: 125.1638 | dt: 528.67ms | tok/sec: 15495.37\n",
      "step 2 | loss: 89.2411 | dt: 532.10ms | tok/sec: 15395.54\n",
      "step 3 | loss: 89.6816 | dt: 535.72ms | tok/sec: 15291.51\n",
      "step 4 | loss: 86.6060 | dt: 529.72ms | tok/sec: 15464.65\n"
     ]
    }
   ],
   "source": [
    "# changing \n",
    "'''\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "'''\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4997e3cf-3a55-4869-9968-302e8f308537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([ 0.0305,  0.0203, -0.0315,  0.0049, -0.0312], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 10 batches\n",
      "step 0 | loss: 433.2917 | dt: 3884.80ms | tok/sec: 8434.92\n",
      "step 1 | loss: 124.0382 | dt: 2216.75ms | tok/sec: 14781.97\n",
      "step 2 | loss: 89.7909 | dt: 2138.37ms | tok/sec: 15323.81\n",
      "step 3 | loss: 86.3727 | dt: 2136.66ms | tok/sec: 15336.08\n",
      "step 4 | loss: 85.2940 | dt: 2126.21ms | tok/sec: 15411.43\n"
     ]
    }
   ],
   "source": [
    "# increasing the batch size x2\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "# trial batch size 16 - same speed like before awesome\n",
    "# trial batch size 32 - same speed like before awesome\n",
    "# trial batch size 64 - hanging after 1st iteration.\n",
    "\n",
    "train_loader = DataLoaderLite(B=32, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a32efd8-d693-493d-89dd-b0455a6bf97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0129,  0.0052, -0.0114,  0.0248,  0.0078], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "→ Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 20 batches\n",
      "step 0 | loss: 109.6663 | dt: 1022.70ms | tok/sec: 16020.29\n",
      "step 1 | loss: 108.7750 | dt: 1019.81ms | tok/sec: 16065.69\n",
      "step 2 | loss: 108.2115 | dt: 1022.03ms | tok/sec: 16030.81\n",
      "step 3 | loss: 109.1347 | dt: 1070.70ms | tok/sec: 15302.21\n",
      "step 4 | loss: 31.8584 | dt: 1020.77ms | tok/sec: 16050.56\n",
      "step 5 | loss: 32.9130 | dt: 1028.16ms | tok/sec: 15935.28\n",
      "step 6 | loss: 34.3935 | dt: 1026.78ms | tok/sec: 15956.70\n",
      "step 7 | loss: 33.2719 | dt: 1069.57ms | tok/sec: 15318.26\n",
      "step 8 | loss: 21.2987 | dt: 1031.79ms | tok/sec: 15879.20\n",
      "step 9 | loss: 21.3444 | dt: 1013.05ms | tok/sec: 16172.91\n"
     ]
    }
   ],
   "source": [
    "# increasing the batch size x2 + grad_accum = 4 \n",
    "''' NOT MUCH difference'''\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "# trial batch size 16 - same speed like before awesome\n",
    "# trial batch size 32 - same speed like before awesome\n",
    "# trial batch size 64 - hanging after 1st iteration.\n",
    "\n",
    "train_loader = DataLoaderLite(B=16, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "grad_accum = 4\n",
    "optimizer.zero_grad()\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss/grad_accum\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    #optimizer.step()\n",
    "\n",
    "    if ((i+1) % grad_accum) == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3614323-9f7e-4f20-b567-f9faf823e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increasing the batch size x2 + grad_accum = 4 \n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ✅ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"→ Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device)\n",
    "model = model.to(memory_format=torch.channels_last)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# load weights if needed\n",
    "# missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "# trial batch size 16 - same speed like before awesome\n",
    "# trial batch size 32 - same speed like before awesome\n",
    "# trial batch size 64 - hanging after 1st iteration.\n",
    "\n",
    "train_loader = DataLoaderLite(B=16, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "grad_accum = 4\n",
    "optimizer.zero_grad()\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x = x.to(device,, memory_format=torch.channels_last)\n",
    "    y = y.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # NEW CODE ADDED HERE\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss/grad_accum\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    #optimizer.step()\n",
    "\n",
    "    if ((i+1) % grad_accum) == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    print(f\"step {i} | loss: {loss.item():.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23ef70e0-d97b-4ea3-8c5a-61f654e5edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62e6a695-53e1-4a20-b537-6f6d0674945b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                      aten::empty         1.97%       1.424ms         1.97%       1.424ms       6.748us         480 B         480 B           211  \n",
      "                                  aten::embedding         0.06%      43.329us         0.44%     317.185us     317.185us           0 B           0 B             1  \n",
      "                                    aten::reshape         1.65%       1.191ms         4.61%       3.335ms       8.509us           0 B           0 B           392  \n",
      "                                       aten::view         3.44%       2.488ms         3.44%       2.488ms       5.480us           0 B           0 B           454  \n",
      "                               aten::index_select         0.07%      54.160us         0.36%     256.688us     256.688us           0 B           0 B             1  \n",
      "                                    aten::resize_         0.29%     206.752us         0.29%     206.752us       6.669us           0 B           0 B            31  \n",
      "                                     aten::expand         0.57%     415.296us         0.73%     524.576us       8.600us           0 B           0 B            61  \n",
      "                                 aten::as_strided         2.85%       2.061ms         2.85%       2.061ms       1.756us           0 B           0 B          1174  \n",
      "                                     aten::gather         0.16%     116.672us         0.22%     156.336us     156.336us           0 B           0 B             1  \n",
      "                                 cudaLaunchKernel        14.09%      10.188ms        14.09%      10.188ms       9.121us           0 B           0 B          1117  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 72.289ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model\n",
    "inputs = x\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30a6b031-d008-428f-91b5-ccd51d30d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         1.10%       2.481ms         3.05%       6.882ms      32.616us      90.088ms        38.83%     109.423ms     518.594us           211   2203318.223  \n",
      "                                              aten::mul         0.95%       2.140ms         2.86%       6.450ms      21.357us      50.611ms        21.81%      53.287ms     176.447us           302      1330.674  \n",
      "                                              aten::add         0.56%       1.268ms         1.14%       2.566ms      14.179us      21.746ms         9.37%      22.645ms     125.113us           181       472.359  \n",
      "                                        aten::embedding         0.01%      31.281us         0.11%     246.641us     246.641us       0.000us         0.00%     156.800us     156.800us             1            --  \n",
      "                                          aten::reshape         0.27%     613.361us         0.74%       1.681ms       4.287us       0.000us         0.00%       0.000us       0.000us           392            --  \n",
      "                                             aten::view         0.55%       1.238ms         0.55%       1.238ms       2.728us       0.000us         0.00%       0.000us       0.000us           454            --  \n",
      "                                     aten::index_select         0.01%      31.664us         0.09%     205.760us     205.760us       0.000us         0.00%     156.800us     156.800us             1            --  \n",
      "                                            aten::empty         0.34%     776.369us         0.34%     776.369us       3.679us       0.000us         0.00%       0.000us       0.000us           211            --  \n",
      "                                          aten::resize_         0.04%      94.801us         0.04%      94.801us       3.058us       0.000us         0.00%       0.000us       0.000us            31            --  \n",
      "                                           aten::expand         0.14%     306.688us         0.16%     368.640us       6.043us       0.000us         0.00%       0.000us       0.000us            61            --  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 225.758ms\n",
      "Self CUDA time total: 232.032ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    with_flops=True\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"flops\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4d3db-a0cc-44f1-94f7-e7f5f0823f5a",
   "metadata": {},
   "source": [
    "The Memory and Compute Requirement notebook¶\n",
    "This notebook is a simple tool to estimate the memory and compute requirements for the solution of the problem. The notebook is divided into two sections: the first one is dedicated to the memory requirements, while the second one is dedicated to the compute requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96921a6f-bb31-4c0e-ae14-6710e203d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some working statistics\n",
    "\n",
    "# Model\n",
    "vocabulary_size = 50257\n",
    "embedding_dimensions = 768\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "feed_forward_ratio = 4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parameter_counts = {\n",
    "    \"Token Embeddings\": vocabulary_size * embedding_dimensions,\n",
    "    \"Attention\": (\n",
    "        embedding_dimensions * 3 * embedding_dimensions + embedding_dimensions**2\n",
    "    ) * num_hidden_layers,\n",
    "    \"MLP\": (\n",
    "        embedding_dimensions * feed_forward_ratio * embedding_dimensions * 2\n",
    "    ) * num_hidden_layers,\n",
    "    \"Norm\": embedding_dimensions * 2 * num_hidden_layers + embedding_dimensions,\n",
    "    \"Output\": 0, # We share the embedding weights\n",
    "}\n",
    "\n",
    "plt.bar(parameter_counts.keys(), parameter_counts.values())\n",
    "\n",
    "plt.title(\"Model Parameters\")\n",
    "plt.ylabel(\"# of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Total parameters: {total_parameter_count:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

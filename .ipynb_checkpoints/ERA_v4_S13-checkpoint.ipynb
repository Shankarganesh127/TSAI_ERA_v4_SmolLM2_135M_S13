{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "442675b1-7304-4ee8-abc7-e866d3c3a08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(architectures=['LlamaForCausalLM'],\n",
       "          attention_bias=False,\n",
       "          attention_dropout=0.0,\n",
       "          bos_token_id=0,\n",
       "          eos_token_id=0,\n",
       "          hidden_act='silu',\n",
       "          hidden_size=576,\n",
       "          initializer_range=0.041666666666666664,\n",
       "          intermediate_size=1536,\n",
       "          is_llama_config=True,\n",
       "          max_position_embeddings=8192,\n",
       "          model_type='llama',\n",
       "          num_attention_heads=9,\n",
       "          num_hidden_layers=30,\n",
       "          num_key_value_heads=3,\n",
       "          pretraining_tp=1,\n",
       "          rms_norm_eps=1e-05,\n",
       "          rope_interleaved=False,\n",
       "          rope_scaling=None,\n",
       "          rope_theta=100000,\n",
       "          tie_word_embeddings=True,\n",
       "          torch_dtype='bfloat16',\n",
       "          transformers_version='4.40.1',\n",
       "          use_cache=True,\n",
       "          vocab_size=49152)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, types\n",
    "\n",
    "CONFIG_PATH = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\"   # <- change this\n",
    "MODEL_DIR   = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"              # folder containing model.safetensors\n",
    "\n",
    "with open(CONFIG_PATH, \"r\") as f:\n",
    "    cfg_dict = json.load(f)\n",
    "\n",
    "# turn into attribute-style object\n",
    "config = types.SimpleNamespace(**cfg_dict)\n",
    "\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b3c945-79a0-4ac5-b4f2-b154abcb724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f2e3a6-15c8-4e47-bf98-4e1feda89313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n",
      "/home/shankar7ganesh/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttentionSDPA(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rope): RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "Before: tensor([-0.0277,  0.0238,  0.0376,  0.0012,  0.0347], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "After: tensor([-0.0894,  0.1367, -0.1045, -0.7188,  1.1797], device='cuda:0',\n",
      "       dtype=torch.bfloat16, grad_fn=<SliceBackward0>)\n",
      "Missing keys: ['lm_head.weight']\n",
      "Unexpected keys: []\n",
      "‚Üí Tied lm_head.weight to embed_tokens.weight\n",
      "Example missing: ['lm_head.weight']\n",
      "Example unexpected: []\n"
     ]
    }
   ],
   "source": [
    "# increasing the batch size x2 + grad_accum = 4 \n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.nn.functional.rms_norm(\n",
    "            x, \n",
    "            normalized_shape=(x.size(-1),),\n",
    "            weight=self.weight,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    HF-style RoPE, theta = rope_theta.\n",
    "    Applies RoPE to first head_dim dims (which is full head_dim in LLaMA).\n",
    "    \"\"\"\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len, device, dtype):\n",
    "        # positions: (seq_len,)\n",
    "        t = torch.arange(seq_len, device=device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, head_dim/2)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)            # (T, head_dim)\n",
    "        sin = emb.sin()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        cos = emb.cos()[None, None, :, :]                 # (1,1,T,head_dim)\n",
    "        return sin.to(dtype), cos.to(dtype)\n",
    "\n",
    "\n",
    "def apply_rope(x, sin, cos):\n",
    "    \"\"\"\n",
    "    x: (B, n_heads, T, head_dim)\n",
    "    sin/cos: (1,1,T,head_dim)\n",
    "    \"\"\"\n",
    "    x1 = x[..., ::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    # rotate pairs\n",
    "    x_rot = torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
    "    return x * cos + x_rot * sin\n",
    "\n",
    "class LlamaAttentionSDPA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads         # Hq\n",
    "        self.num_kv_heads = config.num_key_value_heads      # Hkv\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rope = RotaryEmbedding(self.head_dim, base=config.rope_theta)\n",
    "        self.attn_dropout = float(getattr(config, \"attention_dropout\", 0.0))\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "        device, dtype = x.device, x.dtype\n",
    "\n",
    "        # Project to Q, K, V  -> (B,T,H,D)\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "        v = self.v_proj(x).reshape(B, T, self.num_kv_heads, self.head_dim)\n",
    "\n",
    "        # RoPE wants (B,H,T,D)\n",
    "        sin, cos = self.rope(T, device=device, dtype=dtype)\n",
    "        q = apply_rope(q.transpose(1, 2), sin, cos)  # (B,Hq,T,D)\n",
    "        k = apply_rope(k.transpose(1, 2), sin, cos)  # (B,Hkv,T,D)\n",
    "        v = v.transpose(1, 2)                        # (B,Hkv,T,D)  ‚úÖ IMPORTANT\n",
    "\n",
    "        # GQA: expand K,V heads to match Q heads\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            repeat_factor = self.num_heads // self.num_kv_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B,Hq,T,D)\n",
    "\n",
    "        # SDPA fused causal attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.attn_dropout if self.training else 0.0,\n",
    "            is_causal=True,\n",
    "        )  # (B,Hq,T,D)\n",
    "\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        return self.o_proj(out)\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.up_proj   = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP = SiLU(gate_proj(x)) * up_proj(x) then down_proj\n",
    "        return self.down_proj(self.act(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self_attn = LlamaAttentionSDPA(config)\n",
    "        #self.self_attn = LlamaAttention(config)\n",
    "        self.mlp = LlamaMLP(config)\n",
    "\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # attention\n",
    "        h = x + self.self_attn(self.input_layernorm(x), attention_mask=attention_mask)\n",
    "        # mlp\n",
    "        out = h + self.mlp(self.post_attention_layernorm(h))\n",
    "        return out\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embed_tokens(input_ids)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class LlamaForCausalLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = LlamaModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # tie embeddings if needed\n",
    "        if getattr(config, \"tie_word_embeddings\", False):\n",
    "            self.lm_head.weight = self.model.embed_tokens.weight\n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None):\n",
    "        # transformer hidden states\n",
    "        h = self.model(input_ids, attention_mask=attention_mask)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # INFERENCE PATH\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        # TRAINING PATH\n",
    "        # shift for autoregressive loss\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1),\n",
    "        )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.bfloat16 if config.torch_dtype == \"bfloat16\" else torch.float16\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "\n",
    "model = LlamaForCausalLM(config).to(device=device, dtype=dtype)\n",
    "print (model.eval())\n",
    "\n",
    "sum(p.numel() for p in model.parameters()) / 1e6\n",
    "\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "weights_path = os.path.join(MODEL_DIR, \"model.safetensors\")\n",
    "state = load_file(weights_path)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"Before:\", layer.view(-1)[:5])\n",
    "\n",
    "# load into our model\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "\n",
    "layer = model.model.layers[0].self_attn.q_proj.weight\n",
    "\n",
    "print(\"After:\", layer.view(-1)[:5])\n",
    "\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "# Fix for tied embeddings\n",
    "if \"lm_head.weight\" in missing:\n",
    "    model.lm_head.weight = model.model.embed_tokens.weight\n",
    "    print(\"‚Üí Tied lm_head.weight to embed_tokens.weight\")\n",
    "\n",
    "# print a few to debug if any mismatch\n",
    "print(\"Example missing:\", missing[:10])\n",
    "print(\"Example unexpected:\", unexpected[:10])\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER_DIR = \"models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/\"  # where tokenizer.json is\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n",
    "\n",
    "with open(\"input.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer_path=TOKENIZER_DIR):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "        # load raw text\n",
    "        with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # tokenize entire dataset at once\n",
    "        tokens = self.tokenizer(text).input_ids\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # pointer\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        # get (B*T) + 1 tokens for labels\n",
    "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
    "\n",
    "        # reset if we hit the end\n",
    "        if len(buf) < B*T + 1:\n",
    "            self.current_position = 0\n",
    "            buf = self.tokens[: B*T + 1]\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # (B, T)\n",
    "        y = buf[1:].view(B, T)   # (B, T)\n",
    "\n",
    "        self.current_position += B*T\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efb7a63-d8ec-40cf-be2e-b3e6bb685b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 20 batches\n",
      "‚û°Ô∏è Starting training from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñâ         | 499/5000 [06:28<57:55,  1.30it/s, loss=0.9962, lr=0.000300, tok/s=27851.7, dt(ms)=588.26]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 499): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|‚ñà         | 500/5000 [06:29<1:22:40,  1.10s/it, loss=0.9962, lr=0.000300, tok/s=27851.7, dt(ms)=588.26]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|‚ñà‚ñâ        | 999/5000 [12:57<51:31,  1.29it/s, loss=0.5026, lr=0.000298, tok/s=27571.7, dt(ms)=594.23]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 999): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|‚ñà‚ñà        | 1000/5000 [12:58<1:13:17,  1.10s/it, loss=0.5026, lr=0.000298, tok/s=27571.7, dt(ms)=594.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_1000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|‚ñà‚ñà‚ñâ       | 1499/5000 [19:26<45:10,  1.29it/s, loss=0.0528, lr=0.000296, tok/s=27382.4, dt(ms)=598.34]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 1499): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|‚ñà‚ñà‚ñà       | 1500/5000 [19:27<1:03:02,  1.08s/it, loss=0.0528, lr=0.000296, tok/s=27382.4, dt(ms)=598.34]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_1500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|‚ñà‚ñà‚ñà‚ñâ      | 1999/5000 [25:54<38:36,  1.30it/s, loss=0.0192, lr=0.000293, tok/s=27700.5, dt(ms)=591.47]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 1999): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 2000/5000 [25:55<53:46,  1.08s/it, loss=0.0192, lr=0.000293, tok/s=27700.5, dt(ms)=591.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_2000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 2499/5000 [32:23<32:08,  1.30it/s, loss=0.0129, lr=0.000289, tok/s=27894.7, dt(ms)=587.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 2499): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2500/5000 [32:24<44:25,  1.07s/it, loss=0.0129, lr=0.000289, tok/s=27894.7, dt(ms)=587.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_2500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 2999/5000 [38:52<25:48,  1.29it/s, loss=0.0104, lr=0.000284, tok/s=27902.0, dt(ms)=587.20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 2999): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3000/5000 [38:53<35:54,  1.08s/it, loss=0.0104, lr=0.000284, tok/s=27902.0, dt(ms)=587.20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_3000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 3499/5000 [45:21<19:19,  1.29it/s, loss=0.0090, lr=0.000278, tok/s=27653.7, dt(ms)=592.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 3499): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 3500/5000 [45:22<27:15,  1.09s/it, loss=0.0090, lr=0.000278, tok/s=27653.7, dt(ms)=592.47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_3500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 3999/5000 [51:49<12:54,  1.29it/s, loss=0.0081, lr=0.000271, tok/s=28186.1, dt(ms)=581.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 3999): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4000/5000 [51:50<17:50,  1.07s/it, loss=0.0081, lr=0.000271, tok/s=28186.1, dt(ms)=581.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_4000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 4499/5000 [58:18<06:28,  1.29it/s, loss=0.0075, lr=0.000264, tok/s=27964.3, dt(ms)=585.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 4499): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 4500/5000 [58:19<09:08,  1.10s/it, loss=0.0075, lr=0.000264, tok/s=27964.3, dt(ms)=585.89]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_4500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 4999/5000 [1:04:47<00:00,  1.30it/s, loss=0.0073, lr=0.000256, tok/s=27943.3, dt(ms)=586.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Sample (step 4999): The meaning of life is  to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [1:04:48<00:00,  1.29it/s, loss=0.0073, lr=0.000256, tok/s=27943.3, dt(ms)=586.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved checkpoint: checkpoints/step_5000.pt\n",
      "üéâ Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5000)\n",
    "\n",
    "# ‚úÖ TensorBoard Writer\n",
    "writer = SummaryWriter(log_dir=\"runs/llama_training\")\n",
    "\n",
    "train_loader = DataLoaderLite(B=16, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "grad_accum = 4\n",
    "max_steps = 5000\n",
    "save_interval = 500\n",
    "predict_interval = 500\n",
    "\n",
    "# ------------------------------\n",
    "# RESUME TRAINING (if exists)\n",
    "# ------------------------------\n",
    "\n",
    "RESUME_FROM = f\"{CHECKPOINT_DIR}/latest.pt\"\n",
    "start_step = 0\n",
    "\n",
    "if os.path.exists(RESUME_FROM):\n",
    "    ckpt = torch.load(RESUME_FROM, map_location=\"cuda\")\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    start_step = ckpt[\"step\"]\n",
    "\n",
    "    torch.set_rng_state(ckpt[\"cpu_rng\"].cpu())\n",
    "    torch.cuda.set_rng_state_all(ckpt[\"cuda_rng\"])\n",
    "\n",
    "    print(f\"üîÑ Resumed from step: {start_step}\")\n",
    "else:\n",
    "    print(\"‚û°Ô∏è Starting training from scratch\")\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# ------------------------------\n",
    "# TRAIN LOOP\n",
    "# ------------------------------\n",
    "\n",
    "pbar = tqdm(range(start_step, max_steps), desc=\"Training\")\n",
    "\n",
    "for step in pbar:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if ((step + 1) % grad_accum) == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    dt_ms = (time.time() - t0) * 1000\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / (dt_ms / 1000)\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # ----------------- TensorBoard Logging -----------------\n",
    "    writer.add_scalar(\"loss/train\", loss.item(), step)\n",
    "    writer.add_scalar(\"learning_rate\", current_lr, step)\n",
    "    writer.add_scalar(\"tokens_per_sec\", tokens_per_sec, step)\n",
    "\n",
    "    # --- tqdm progress bar display ---\n",
    "    pbar.set_postfix({\n",
    "        \"loss\": f\"{loss.item():.4f}\",\n",
    "        \"lr\": f\"{current_lr:.6f}\",\n",
    "        \"tok/s\": f\"{tokens_per_sec:.1f}\",\n",
    "        \"dt(ms)\": f\"{dt_ms:.2f}\"\n",
    "    })\n",
    "\n",
    "    # ---------------------- TEXT GENERATION ---------------------------\n",
    "    if (step + 1) % predict_interval == 0:\n",
    "        model.eval()\n",
    "        prompt = \"The meaning of life is\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids)[0]\n",
    "            next_token = out[:, -1].argmax(-1)\n",
    "            text = tokenizer.decode(next_token)\n",
    "\n",
    "        writer.add_text(\"generation/sample\", f\"{prompt} {text}\", step)\n",
    "        print(f\"\\nüìù Sample (step {step}): {prompt} {text}\")\n",
    "        model.train()\n",
    "\n",
    "    # ------------------------- CHECKPOINTING ---------------------------\n",
    "    if (step + 1) % save_interval == 0:\n",
    "        ckpt_path = f\"{CHECKPOINT_DIR}/step_{step+1}.pt\"\n",
    "\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"step\": step + 1,\n",
    "            \"cpu_rng\": torch.get_rng_state(),\n",
    "            \"cuda_rng\": torch.cuda.get_rng_state_all(),\n",
    "        }, ckpt_path)\n",
    "\n",
    "        print(f\"\\nüíæ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"üéâ Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac4fc47-00b4-4e49-8f5c-0bfa58f4e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 20 batches\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RNG state must be a torch.ByteTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m     start_step = ckpt[\u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     44\u001b[39m     torch.set_rng_state(ckpt[\u001b[33m\"\u001b[39m\u001b[33mcpu_rng\u001b[39m\u001b[33m\"\u001b[39m].cpu())\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_rng_state_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda_rng\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Resumed from step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/random.py:89\u001b[39m, in \u001b[36mset_rng_state_all\u001b[39m\u001b[34m(new_states)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Set the random number generator state of all devices.\u001b[39;00m\n\u001b[32m     84\u001b[39m \n\u001b[32m     85\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    new_states (Iterable of torch.ByteTensor): The desired state for each device.\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(new_states):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[43mset_rng_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/random.py:79\u001b[39m, in \u001b[36mset_rng_state\u001b[39m\u001b[34m(new_state, device)\u001b[39m\n\u001b[32m     76\u001b[39m     default_generator = torch.cuda.default_generators[idx]\n\u001b[32m     77\u001b[39m     default_generator.set_state(new_state)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:341\u001b[39m, in \u001b[36m_lazy_call\u001b[39m\u001b[34m(callable, **kwargs)\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _initialization_lock:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    343\u001b[39m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[32m    344\u001b[39m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[32m    345\u001b[39m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[32m    346\u001b[39m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/TSAI_ERA_v4_SmolLM_135M_S13/.venv/lib/python3.12/site-packages/torch/cuda/random.py:77\u001b[39m, in \u001b[36mset_rng_state.<locals>.cb\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     75\u001b[39m     idx = current_device()\n\u001b[32m     76\u001b[39m default_generator = torch.cuda.default_generators[idx]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[43mdefault_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: RNG state must be a torch.ByteTensor"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5000)\n",
    "\n",
    "train_loader = DataLoaderLite(B=16, T=1024, tokenizer_path=TOKENIZER_DIR)\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "grad_accum = 4\n",
    "max_steps = 5500\n",
    "save_interval = 500\n",
    "predict_interval = 500\n",
    "\n",
    "start_step = 0\n",
    "\n",
    "# ------------------------------\n",
    "# BUILD MODEL\n",
    "# ------------------------------\n",
    "#model = LlamaForCausalLM(config).to(device) # already instanced\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "# ------------------------------\n",
    "# RESUME TRAINING (if exists)\n",
    "# ------------------------------\n",
    "# ------------------------------\n",
    "# RESUME TRAINING (SAFE VERSION)\n",
    "# ------------------------------\n",
    "RESUME_FROM = f\"{CHECKPOINT_DIR}/step_5000.pt\"\n",
    "start_step = 0\n",
    "\n",
    "if os.path.exists(RESUME_FROM):\n",
    "\n",
    "    ckpt = torch.load(RESUME_FROM, map_location=\"cuda\")\n",
    "    print(f\"üîÑ Loading checkpoint: {RESUME_FROM}\")\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
    "    start_step = ckpt[\"step\"]\n",
    "\n",
    "    # ------------------------------\n",
    "    # SAFE RNG RESTORE\n",
    "    # ------------------------------\n",
    "\n",
    "    # CPU RNG\n",
    "    cpu_rng = ckpt[\"cpu_rng\"]\n",
    "    if cpu_rng.device != torch.device(\"cpu\"):\n",
    "        cpu_rng = cpu_rng.cpu()\n",
    "    torch.set_rng_state(cpu_rng)\n",
    "\n",
    "    # CUDA RNG ‚Äî FIX FOR YOUR ERROR\n",
    "    cuda_rng_list = ckpt[\"cuda_rng\"]\n",
    "    \n",
    "    # ensure list\n",
    "    if isinstance(cuda_rng_list, torch.Tensor):\n",
    "        cuda_rng_list = [cuda_rng_list]\n",
    "\n",
    "    safe_cuda_states = []\n",
    "    for s in cuda_rng_list:\n",
    "        if not isinstance(s, torch.ByteTensor):\n",
    "            s = s.to(torch.uint8)\n",
    "        if s.device.type != \"cpu\":\n",
    "            s = s.cpu()\n",
    "        safe_cuda_states.append(s)\n",
    "\n",
    "    torch.cuda.set_rng_state_all(safe_cuda_states)\n",
    "\n",
    "    print(f\"üîÑ Resumed from step {start_step}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚û°Ô∏è Starting training from scratch\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# ------------------------------\n",
    "# TRAINING LOOP (with timing + throughput)\n",
    "# ------------------------------\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "pbar = tqdm(range(start_step, max_steps), desc=\"Training\")\n",
    "\n",
    "for step in pbar:\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply optimizer step only every grad_accum steps\n",
    "    if ((step + 1) % grad_accum) == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "    # --- SPEED METRICS ---\n",
    "    dt_ms = (time.time() - t0) * 1000\n",
    "    tokens_processed = train_loader.B * train_loader.T\n",
    "    tokens_per_sec = tokens_processed / (dt_ms / 1000)\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    # --- TensorBoard Logging ---\n",
    "    writer.add_scalar(\"loss/train\", loss.item(), step)\n",
    "    writer.add_scalar(\"learning_rate\", current_lr, step)\n",
    "    writer.add_scalar(\"tokens_per_sec\", tokens_per_sec, step)\n",
    "\n",
    "    # --- tqdm progress bar display ---\n",
    "    pbar.set_postfix({\n",
    "        \"loss\": f\"{loss.item():.4f}\",\n",
    "        \"lr\": f\"{current_lr:.6f}\",\n",
    "        \"tok/s\": f\"{tokens_per_sec:.1f}\",\n",
    "        \"dt(ms)\": f\"{dt_ms:.2f}\"\n",
    "    })\n",
    "\n",
    "    # ---------------------- TEXT GENERATION ---------------------------\n",
    "    if (step + 1) % predict_interval == 0:\n",
    "        model.eval()\n",
    "        prompt = \"The meaning of life is\"\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids)[0]\n",
    "            next_token = out[:, -1].argmax(-1)\n",
    "            text = tokenizer.decode(next_token)\n",
    "\n",
    "        writer.add_text(\"generation/sample\", f\"{prompt} {text}\", step)\n",
    "        print(f\"\\nüìù Sample (step {step}): {prompt} {text}\")\n",
    "        model.train()\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # ------------------------- CHECKPOINTING ---------------------------\n",
    "    if (step + 1) % save_interval == 0:\n",
    "        ckpt_path = f\"{CHECKPOINT_DIR}/step_{step+1}.pt\"\n",
    "\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"step\": step + 1,\n",
    "            \"cpu_rng\": torch.get_rng_state(),\n",
    "            \"cuda_rng\": torch.cuda.get_rng_state_all(),\n",
    "        }, ckpt_path)\n",
    "\n",
    "        print(f\"\\nüíæ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"üéâ Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0273af-75c4-416f-9a65-40c10121ac00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
